# 딥러닝 기본 용어 개념

## 01. 가중치 

* 입력 신호의 영향력을 제어한다. 보통 w로 표현한다.



## 02. 편향

* 뉴런이 "얼마나 쉽게 활성화를 할 것인가"를 제어한다



## 03. 활성화 함수

* 입력 신호의 총합을 출력 신호로 변환하는 함수
* '활성화'라는 이름이 말해주듯 입력 신호의 총합이 활성화를 일으키는지를 정하는 함수

```
h(x) = 0 (x <=0)
	   1 (x > 0)
```

이 아래의 식을 분석하면

```
a = b + w1*x1 + w2*x2
y = h(a)
```

* w1,w2는 x1,x2 입력값에 곱해지면서 입력 신호의 영향력을 제어하고, b가 얼마나 쉽게 활성화 할 것인지를 제어했다.
* h(a) 함수를 통해 입력 신호의 총합이 활성화를 일으킬지 정한다.
* 활성화 함수의 가장 큰 특징은 비선형 함수여야 한다. ex) sigmoid, ReLU



## 04. forward

* h(x) 식에 입력 값 x로 부터 예측된  y 값을 얻는 것을 말한다.

  ```python	
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서 h = model(x_train) 이 과정이 forward 연산이다.




## 05. backward

* 학습 과정에서 cost function을 미분하여 기울기를 구하는 것을 말한다

  ```python
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서  cost.backward()는 비용 함수의 기울기를 구하는 의미이며 같은 말로는 backward 연산이다.
  
  

## 06. iteration

* 한 번의 epoch 내 에서 매개변수(W,b)가 업데이트 되는 횟수를 말한다.
* batch 경사 하강법인 경우 iteration은 1이 된다.
* mini batch 경사 하강법인 경우 iteration은 총 데이터 수/mini batch의 크기가 된다.
* SGD 경사 하강법인 경우 iteration은 총 데이터 수가 된다.



## 07. 로지스틱 회귀

* 이진 분류(Binary Classification)를 위한 대표적인 알고리즘이다

> 선형 회귀의 경우 H(x) = Wx +b 식이 성립 했지만, 로지스틱 회귀에서는 S자 그래프를 만드는 함수 f(x)를 추가하여 H(x) = f(Wx+b) 로 나타낸다. 이 S자 그래프를 만드는 함수 f(X)는 널리 알려진 시그모이드 함수를 말한다



## 08. 퍼셉트론

퍼셉트론은 단층 퍼셉트론과 다층 퍼셉트론이 있다.

쉽게 생각할 수 있듯이 단층 퍼셉트론은 층이 1개인 퍼셉트론이고 다층 퍼셉트론은 층이 2개인 퍼셉트론을 말한다.

여기서 헷갈리는 점은 3개 이상의 층을 가진 퍼셉트론을 다층 퍼셉트론이 아니라 심층 신경망(Deep Neural Network,DNN)이라고 한다.

이 심층 신경망은 다층 퍼셉트론만 이야기 하는 것이 아니라, 여러 변형된 다양한 신경망들고 은닉층이 2개 이상이라면 심층 신경망이라고 한다.

퍼셉트론의 가중치 W를 인간이 수동으로 찾는 것에서 기계가 스스로 가중치를 찾아내도록 자동화 시키게 된다면, 이것이 머신 러닝에서 말하는 학습(training) 이다.

보통은 손실 함수와 옵티마이저를 이용하여 학습을 진행합니다. 그리고 만약 학습을 시키는 인공 신경망이 심층 신경망일 경우 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝(Deep Learning)이라고 합니다.



## 09. 과적합과 과소적합

과적합이란 훈련 데이터를 과하게 학습한 경우를 말한다.

훈련 데이터는 실제로 존재하는 많은 데이터의 일부에 불과하기 때문에, 훈련 데이터에 대해서만 과하게 학습한다면 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 생긴다.

예를 들어 검은색 강아지 사진만 훈련 한다면, 나중에 가서는 흰색 강아지나, 갈색 강아지를 보고도 강아지가 아니라고 판단하게 된다.

과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생합니다.

반대로 과소적합은 훈련 자체가 부족한 상태를 말하는데, 과적합과는 달리 훈련 데이터에 대해서도 정확도가 낮다.

과적합을 막기 위해서는 Dropout과 조기종료(Early Stopping)과 같은 방법이 존재한다.



## 10. CNN(Convolution Neural Network)



CNN은 합성곱 층와 풀링층으로 구성된다.

이미지를 입력층에 넣기위해 1차원으로 변환하게 되면, 이전에 가지고 있던 공간적인 구조 정보가 유실되게 된다.

여기서 공간적인 구조 정보라는 것은 거리가 가까운 어떠 픽셀들끼리의 연관성을 포함한다.

결국 이미지의 공간적인 구조 정보를 보존하면서 학습하는 방법이 필요해졌고, 이를 위해 사용하는 것이 CNN(합성곱 신경망)이다.



### 채널

 CNN의 기본 용어인 채널에 대해서 간단히 정의한다.

기계는 이미지보다 숫자(텐서)를 더 잘 처리할 수 있다. 이미지는 높이, 너비, 채널을 가지는 3차원 텐서다.

여기서 채널은 색 성분을 의미한다.

흑백 이미지는 채널 수가 1이고. 컬러 이미지의 경우 빨간색, 초록색, 파란색 채널 수가 3개다.

하나의 픽셀은 이 세 가지 색깔, 삼원색의 조합으로 이루어지기 떄문에 이미지의 텐서는 (28x28x3)의 크기를 가지는 3차원 텐서가 된다. 

채널은 떄로는 깊이라고도 한다.



### 합성곱 연산

합성곱층은 합성곱 연산을 통해서 이미지의 특징을 추출 하는 역할을 한다.

필터라는 n x m 크기의 행렬로 (높이 x 너비)  크기의 이미지를 처음부터 끝까지 훑으면서 필터에 겹쳐지는 부분의 원소 값을 곱해서 모두 더한 값으로 출력한다.

* 커널(필터라고도 함)은 일반적으로 3 x 3 또는 5 x 5를 사용합니다.

입력 이미지와 커널(필터)을 사용하여 합성곱 연산을 통해 나온 결과를 특성 맵(feature map)이라고 한다.



### 스트라이드

커널(필터)의 이동 범위가 디폴트 값으로는 한 칸이지만, 이를 사용자가 정의할 수 있다.

이러한 이동 범위를 스트라이드(stride)라고 한다.



### 패딩

합성곱 연산의 결과 이미지는 입력 값보다 크기가 작아진다는 특징이 있다. 

예를 들어 5 x 5의 이미지를 3 x 3 커널로 합성곱 연산을 하게 되면 3 x 3의 이미지가(특성 맵) 출력 되게 되는데, 합성곱 연산 이후에도 특성 맵의 크기를 입력 크기와 동일하게 유지하도록 입력 이미지에 패딩(padding)을 실행한다.



### 특성 맵의 크기 계산 방법

 맵 높이 = (입력의 높이 -  커널 높이 + 2*패딩) / 스트라이드 +1

맵 너비 = (입력의 너비 - 커널 너비 + 2*패딩) / 스트라이드 +1



### 다수의 채널을 가진 합성곱 연산(3차원 텐서의 합성곱  연산)

실제 합성곱 연산의 입력은 2차원 텐서가 아니라 다수의 채널을 가진 이미지 또는 특성 맵일 수 있다.

만약 다수의 채널을 가진 입력 데이터를 가지고 합성곱 연산을 한다면 커널의 채널 수도 입력의 채널 수만큼 존재해야 된다. 

 다시 말해서 입력의 채널 수와 커널의 채널 수는 같아야 한다.

합성곱 연산을 채널마다 수행하고, 그 결과를 모두 더하여 최종 특성 맵을 얻습니다.

여기서 주의해야 될 점은 합성곱 연산을 할 떄 사용되는 커널은 3개의 커널이 아니라 3개의 채널을 가진 1개의 커널이라는 점이다.

합성곱 연산의 결과로 얻은 특성 맵의 채널 차원은 RGB채널 등과 같은 컬러의 의미를 담고 있지는 않다.

* 입력 이미지의 채널 수는 색을 의미하고 커널의 채널 수는 입력의 채널 수와 같다.

* 특성 맵(출력 이미지)의 채널 수는 깊이를 의미하고 커널의 개수와 같다.
* 가중치의 매개변수는 총 :  커널 높이 x 커널 너비 x 채널 수 x 커널 수
* 서로 채널 갯수가 같은 삼차원 입력 이미지와 삼차원 커널 하나가 합성곱 연산을 하면 이차원 특성맵이 출력된다.





### 풀링(Pooling)

일반적으로 합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적입니다.

풀링 층에서는 합성곱 연산을 마친 특성 맵을 다운샘플링하여 특성 맵의 크기를 줄이는 풀링 연산이 이루어진다.

* 최대 풀링(max pooling)
* 평균 풀링(average pooling)

풀링 연산은커널(필터)와 스트라이드 개념이 존재한다는 점에서 합성곱 연산과 유사하지만, 합성곱 연산과 차이점은 학습해야 할 가중치가 없으며 연산 후에 채널 수가 변하지 않는다는 점이다.

풀링을 사용하면 특성 맵의 크기가 줄어들어 특성 맵의 가중치의 개수를 줄여준다.



