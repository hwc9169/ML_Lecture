# 딥러닝 기본 용어 개념

## 가중치 

* 입력 신호의 영향력을 제어한다. 보통 w로 표현한다.



## 편향

* 뉴런이 "얼마나 쉽게 활성화를 할 것인가"를 제어한다



## 활성화 함수

* 입력 신호의 총합을 출력 신호로 변환하는 함수
* '활성화'라는 이름이 말해주듯 입력 신호의 총합이 활성화를 일으키는지를 정하는 함수

```
h(x) = 0 (x <=0)
	   1 (x > 0)
```

이 아래의 식을 분석하면

```
a = b + w1*x1 + w2*x2
y = h(a)
```

* w1,w2는 x1,x2 입력값에 곱해지면서 입력 신호의 영향력을 제어하고, b가 얼마나 쉽게 활성화 할 것인지를 제어했다.
* h(a) 함수를 통해 입력 신호의 총합이 활성화를 일으킬지 정한다.



## forward

* h(x) 식에 입력 값 x로 부터 예측된  y 값을 얻는 것을 말한다.

  ```python	
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서 h = model(x_train) 이 과정이 forward 연산이다.

  

  ## backward

* 학습 과정에서 cost function을 미분하여 기울기를 구하는 것을 말한다

  ```python
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서  cost.backward()는 비용 함수의 기울기를 구하는 의미이며 같은 말로는 backward 연산이다.

## iteration

* 한 번의 epoch 내 에서 매개변수(W,b)가 업데이트 되는 횟수를 말한다.
* batch 경사 하강법인 경우 iteration은 1이 된다.
* mini batch 경사 하강법인 경우 iteration은 총 데이터 수/mini batch의 크기가 된다.
* SGD 경사 하강법인 경우 iteration은 총 데이터 수가 된다.



## 로지스틱 회귀

* 이진 분류(Binary Classification)를 위한 대표적인 알고리즘이다

```
선형 회귀의 경우 H(x) = Wx +b 식이 성립 했지만, 로지스틱 회귀에서는 S자 그래프를 만드는 함수 f(x)를 추가하여 H(x) = f(Wx+b) 로 나타낸다. 이 S자 그래프를 만드는 함수 f(X)는 널리 알려진 시그모이드 함수를 말한다
```

