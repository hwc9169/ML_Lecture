# 딥러닝 기본 용어 개념

## 01.가중치 

* 입력 신호의 영향력을 제어한다. 보통 w로 표현한다.



## 02.편향

* 뉴런이 "얼마나 쉽게 활성화를 할 것인가"를 제어한다



## 03.활성화 함수

* 입력 신호의 총합을 출력 신호로 변환하는 함수
* '활성화'라는 이름이 말해주듯 입력 신호의 총합이 활성화를 일으키는지를 정하는 함수

```
h(x) = 0 (x <=0)
	   1 (x > 0)
```

이 아래의 식을 분석하면

```
a = b + w1*x1 + w2*x2
y = h(a)
```

* w1,w2는 x1,x2 입력값에 곱해지면서 입력 신호의 영향력을 제어하고, b가 얼마나 쉽게 활성화 할 것인지를 제어했다.
* h(a) 함수를 통해 입력 신호의 총합이 활성화를 일으킬지 정한다.
* 활성화 함수의 가장 큰 특징은 비선형 함수여야 한다. ex) sigmoid, ReLU



## 04.forward

* h(x) 식에 입력 값 x로 부터 예측된  y 값을 얻는 것을 말한다.

  ```python	
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서 h = model(x_train) 이 과정이 forward 연산이다.

  

  ## 05.backward

* 학습 과정에서 cost function을 미분하여 기울기를 구하는 것을 말한다

  ```python
  model = nn.Linear(1,1)
  optimizer = torch.optim.SGD(model.parameters(),lr=0.01)
  
  epochs = 10000
  for epoch in range(epochs+1):
      y = model(x_train)
      cost = F.mse_loss(y,y_train)
      optimizer.zero_grad()
      cost.backward()
      optimizer.step()
  ```

  위 코드에서  cost.backward()는 비용 함수의 기울기를 구하는 의미이며 같은 말로는 backward 연산이다.

## 06.iteration

* 한 번의 epoch 내 에서 매개변수(W,b)가 업데이트 되는 횟수를 말한다.
* batch 경사 하강법인 경우 iteration은 1이 된다.
* mini batch 경사 하강법인 경우 iteration은 총 데이터 수/mini batch의 크기가 된다.
* SGD 경사 하강법인 경우 iteration은 총 데이터 수가 된다.



## 07.로지스틱 회귀

* 이진 분류(Binary Classification)를 위한 대표적인 알고리즘이다

> 선형 회귀의 경우 H(x) = Wx +b 식이 성립 했지만, 로지스틱 회귀에서는 S자 그래프를 만드는 함수 f(x)를 추가하여 H(x) = f(Wx+b) 로 나타낸다. 이 S자 그래프를 만드는 함수 f(X)는 널리 알려진 시그모이드 함수를 말한다



## 08.퍼셉트론

퍼셉트론은 단층 퍼셉트론과 다층 퍼셉트론이 있다.

쉽게 생각할 수 있듯이 단층 퍼셉트론은 층이 1개인 퍼셉트론이고 다층 퍼셉트론은 층이 2개인 퍼셉트론을 말한다.

여기서 헷갈리는 점은 3개 이상의 층을 가진 퍼셉트론을 다층 퍼셉트론이 아니라 심층 신경망(Deep Neural Network,DNN)이라고 한다.

이 심층 신경망은 다층 퍼셉트론만 이야기 하는 것이 아니라, 여러 변형된 다양한 신경망들고 은닉층이 2개 이상이라면 심층 신경망이라고 한다.

퍼셉트론의 가중치 W를 인간이 수동으로 찾는 것에서 기계가 스스로 가중치를 찾아내도록 자동화 시키게 된다면, 이것이 머신 러닝에서 말하는 학습(training) 이다.

보통은 손실 함수와 옵티마이저를 이용하여 학습을 진행합니다. 그리고 만약 학습을 시키는 인공 신경망이 심층 신경망일 경우 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝(Deep Learning)이라고 합니다.



## 09.과적합과 과소적합

과적합이란 훈련 데이터를 과하게 학습한 경우를 말한다.

훈련 데이터는 실제로 존재하는 많은 데이터의 일부에 불과하기 때문에, 훈련 데이터에 대해서만 과하게 학습한다면 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 생긴다.

예를 들어 검은색 강아지 사진만 훈련 한다면, 나중에 가서는 흰색 강아지나, 갈색 강아지를 보고도 강아지가 아니라고 판단하게 된다.

과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생합니다.

반대로 과소적합은 훈련 자체가 부족한 상태를 말하는데, 과적합과는 달리 훈련 데이터에 대해서도 정확도가 낮다.

과적합을 막기 위해서는 Dropout과 조기종료(Early Stopping)과 같은 방법이 존재한다.





