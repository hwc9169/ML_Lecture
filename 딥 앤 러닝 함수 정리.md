# ML 함수 정리

각 함수가 사용되는 상황을 정리 해보았다.

다만 그 원리나 수식의 관한 설명은 추후에 업데이트할 예정이니 양해 바랍니다.



# 활성화 함수

## 1.Sigmoid 함수

S를 그린다고 하여 Sigmoid라는 이름이 붙여진 함수다.

보통 이진 분류(Binary Classification)문제에서 사용된다. 

은닉층의 활성화 함수로도 사용할 수는 있으나,  기울기 소실(Vanishing Gradient)이 일어난다는 점에서 출력층에서만 사용된다.

은닉층의 활성화 함수는 ReLU함수가 많이 사용된다.



## 2.Softmax 함수

은닉층에서는 ReLU 함수가 사용되는 것이 일반적이지만 그렇다고 시그모이드나 소프트맥스 함수가 사용되지 않는다는 의미는 아니다.

분류 문제를 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용하여 해결한다.

소프트 맥스 함수는 시그모이드 처럼 출력층의 뉴런에서 주로 사용된다.

시그모이드는 이진 분류 문제에 사용된다면, Softmax는 다중 클래스 분류 문제에 주로 사용된다.



## 3.ReLU 함수

Sigmoid 함수에서 발생하는 기울기 소실 문제를 해결하는 해성같은 함수은 ReLU는 은닉층의 활성화 함수로 자주 사용된다.

하지만 ReLU함수에도 단점이 있는데, 입력값이 음수면 기울기가 0이 된다는 점이다.

기울기가 0이 되버리면 그 뉴런은 죽어버리게 되어 다시 희생하기 매우 어렵게 된다. 이문제를 죽은 렐루(dying ReLU)라고 한다.

이를 해결할 수 있는 함수는 바로 Leaky ReLU다



>f(x) = max(0,x)

 

### 1) Leaky ReLU

죽은 렐루를 보완하기 위해 입력 값이 음수일 경우에 0이 아니라 0.001과 같은 작은 수를 반환하도록 한다.



>f(x) = max(ax,x)
>
>a = 0.001



# 손실 함수

## 1.BCELoss

이진 크로스 엔트로피 손실 함수(Binary Cross Entropy Loss)의 약자로 말 그대로 이진 분류 문제에서 사용되는 손실 함수이다. 

보통 출력층의 활성화 함수 시그모이드와 같이 사용된다.



## 2.CrossEntropyLoss()

크로스 엔트로피 손실 함수는 다중 클래스 분류 문제에서 자주 사용된다. 

출력층의 활성화 함수는 소프트 맥스와 자주 사용된다.



* 주의해야 될점은 CrossEntropyLoss 함수는 소프트맥스 함수를 이미 포함하고 있습니다.



## 3.MSE

평균 제곱 오차(Mean Square Error)는 예측 값(output)과 정답(target)을 뺀후 제곱하여 오차를 계산하는 함수다.

이 함수는 회귀 문제에서 사용된다.  그리고 회귀 문제는 출력층의 활성화 함수가 없으므로(선형 함수는 활성화 함수가 아니다) MSE 손실 함수는 출력층에 활성화 함수가 없다고 말 할 수 있다.

